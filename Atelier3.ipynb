{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UzPg77jpDS6",
        "outputId": "8bec6790-0309-4366-9734-a217481c0c84"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Function to scrape text from a given URL\n",
        "def scrape_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Find and extract text content from relevant elements on the webpage\n",
        "    text = \"\"\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        text += paragraph.get_text() + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Function to create the dataset\n",
        "def create_dataset(urls, scores):\n",
        "    texts = []\n",
        "    for url in urls:\n",
        "        texts.append(scrape_text(url))\n",
        "    # Create DataFrame with text and scores\n",
        "    df = pd.DataFrame({'Text (Arabic Language)': texts, 'Score': scores})\n",
        "    return df\n",
        "\n",
        "# URLs of Arabic news websites related to technology\n",
        "urls = [\n",
        "    'https://www.tech-wd.com/wd/2024/04/03/%d8%aa%d8%b3%d8%b1%d9%8a%d8%a8%d8%a7%d8%aa-%d8%aa%d8%b5%d9%85%d9%8a%d9%85-%d8%aa%d8%b7%d8%a8%d9%8a%d9%82-%d8%a7%d9%84%d9%83%d8%a7%d9%85%d9%8a%d8%b1%d8%a7-ios-18/',\n",
        "    'https://www.elfann.com/news/show/1367468/%D8%A8%D8%B9%D8%AF-%D8%BA%D9%8A%D8%A7%D8%A8%D9%8D-%D8%A7%D9%84%D8%A3%D9%85%D9%8A%D8%B1%D8%A9-%D8%B4%D8%A7%D8%B1%D9%84%D9%8A%D9%86-%D9%85%D9%88%D9%86%D8%A7%D9%83%D9%88-%D8%AA%D8%AA%D8%A3%D9%84%D9%91%D9%82-%D8%A8%D8%AC%D9%85%D8%A8%D8%B3%D9%88%D8%AA-%D8%A5%D9%8A%D9%84',\n",
        " ' https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%84%D8%BA%D8%A9_%D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A%D8%A9',\n",
        "   ' https://www.alarabiya.net/',\n",
        "    'https://alarab.co.uk/',\n",
        "    'https://www.dw.com/ar/%D8%A7%D9%84%D8%B1%D8%A6%D9%8A%D8%B3%D9%8A%D8%A9/s-9106',\n",
        "    'https://www.araby.ai/',\n",
        "    'https://www.france24.com/ar/',\n",
        "    'https://www.skynewsarabia.com/'\n",
        "\n",
        "]\n",
        "\n",
        "# Relevance scores for each text (should be between 0 to 10)\n",
        "scores = [10]* len(urls)\n",
        "\n",
        "# Create the dataset\n",
        "dataset = create_dataset(urls, scores)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "dataset.to_csv('arabic_tech_news_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Dataset created and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_4vkizWqdu0",
        "outputId": "5a060ccf-b6e7-4285-ddf3-c8ea93bfa236"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Check if the input is a string\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stop words\n",
        "        stop_words = set(stopwords.words('arabic'))\n",
        "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "        # Stemming\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n",
        "\n",
        "        # Join tokens back into a single string\n",
        "        preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "        return preprocessed_text\n",
        "    else:\n",
        "        return ''  # Return an empty string for non-string inputs\n",
        "\n",
        "\n",
        "print(\"Preprocessing completed and preprocessed dataset saved successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzBa0EdArRRW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FQhAX5Hqr84",
        "outputId": "27e6b1fe-ad29-400a-d540-245c936a7d36"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
        "\n",
        "# Load preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_arabic_tech_news_dataset.csv')\n",
        "\n",
        "# Split dataset into features and labels\n",
        "texts = dataset['Preprocessed Text'].values\n",
        "scores = dataset['Score'].values\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Define model architecture\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_seq_length))\n",
        "    model.add(SimpleRNN(64, return_sequences=True))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Leave-One-Out cross-validation\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Perform cross-validation\n",
        "mse_scores = []\n",
        "for train_index, test_index in loo.split(texts):\n",
        "    # Split data into training and testing sets\n",
        "    train_texts, test_texts = texts[train_index], texts[test_index]\n",
        "    train_scores, test_scores = scores[train_index], scores[test_index]\n",
        "\n",
        "    # Tokenize and pad sequences\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "    test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "    max_seq_length = max(len(seq) for seq in train_sequences)\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "    test_padded = pad_sequences(test_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "    # Create and compile model\n",
        "    model = create_model()\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_padded, train_scores, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mse, _ = model.evaluate(test_padded, test_scores, verbose=0)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "# Compute mean MSE and standard deviation\n",
        "mean_mse = np.mean(mse_scores)\n",
        "std_mse = np.std(mse_scores)\n",
        "\n",
        "print(f\"Mean MSE (RNN, lr={learning_rate}, bs={batch_size}): {mean_mse}, Std MSE: {std_mse}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "9dUZsz5q7jdz",
        "outputId": "582e4ecc-a2ea-4b77-9a76-1780bc867a1f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Bidirectional, LSTM, GRU, Dense\n",
        "\n",
        "# Load preprocessed dataset\n",
        "dataset = pd.read_csv('preprocessed_arabic_tech_news_dataset.csv')\n",
        "\n",
        "# Split dataset into features and labels\n",
        "texts = dataset['Preprocessed Text'].values\n",
        "scores = dataset['Score'].values\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Define model architectures\n",
        "def create_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_seq_length))\n",
        "    model.add(SimpleRNN(64, return_sequences=True))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def create_bidirectional_rnn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_seq_length))\n",
        "    model.add(Bidirectional(SimpleRNN(64, return_sequences=True)))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def create_lstm_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_seq_length))\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def create_gru_model():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_seq_length))\n",
        "    model.add(GRU(64, return_sequences=True))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    return model\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rates = [0.001, 0.01]\n",
        "batch_sizes = [32, 64]\n",
        "epochs = 10\n",
        "\n",
        "# Define cross-validation strategy (e.g., K-Fold)\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation for each architecture and hyperparameter combination\n",
        "architectures = {\n",
        "    'RNN': create_rnn_model,\n",
        "    'Bidirectional_RNN': create_bidirectional_rnn_model,\n",
        "    'LSTM': create_lstm_model,\n",
        "    'GRU': create_gru_model\n",
        "}\n",
        "\n",
        "for architecture, create_model_func in architectures.items():\n",
        "    for learning_rate in learning_rates:\n",
        "        for batch_size in batch_sizes:\n",
        "            model = create_model_func()\n",
        "            model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "            print(f\"Training {architecture} model with learning rate {learning_rate} and batch size {batch_size}\")\n",
        "            scores = cross_val_score(model, texts, scores, cv=kf, scoring='neg_mean_squared_error')\n",
        "            mse_scores = -scores  # Convert negative scores to positive\n",
        "            print(f\"Mean MSE ({architecture}, lr={learning_rate}, bs={batch_size}): {mse_scores.mean()}, Std MSE: {mse_scores.std()}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKjlJpbL_0sa",
        "outputId": "2f99a9a8-013a-4914-b153-f002af1ba91d"
      },
      "outputs": [],
      "source": [
        "pip install pytorch-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "80t_1CyoAh9h",
        "outputId": "e94b1f89-edf2-4ddc-f7aa-f04e16923cc1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained GPT2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Fine-tune the model on your customized dataset\n",
        "# Assuming you have your own dataset in a text file, you need to create a TextDataset\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/CV_Houda_kaissi (1).docx',  # Path to your customized dataset\n",
        "    block_size=128  # Adjust block size according to your dataset\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,  # Adjust number of epochs as needed\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
